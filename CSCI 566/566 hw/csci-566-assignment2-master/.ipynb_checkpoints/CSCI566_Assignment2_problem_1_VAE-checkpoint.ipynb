{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6owH3k8uOMq"
   },
   "source": [
    "# Problem 1 - Variational Auto-Encoder (VAE)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jesbu1/csci-566-assignment2/blob/master/CSCI566_Assignment2_problem_1_VAE.ipynb)\n",
    "\n",
    "\n",
    "Variational Auto-Encoders (VAEs) are a widely used class of generative models. They are simple to implement and, in contrast to other generative model classes like Generative Adversarial Networks (GANs, see Problem 2), they optimize an explicit maximum likelihood objective to train the model. Finally, their architecture makes them well-suited for unsupervised representation learning, i.e. learning low-dimensional representations of high-dimenionsal inputs, like images, with only self-supervised objectives (data reconstruction in the case of VAEs).\n",
    "\n",
    "![VAE_sketch.png](data:image/png;base64,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)\n",
    "*(image source: https://mlexplained.com/2017/12/28/an-intuitive-explanation-of-variational-autoencoders-vaes-part-1)*\n",
    "\n",
    "**By working on this problem you will learn and practice the following steps:**\n",
    "1. Set up a data loading pipeline in PyTorch.\n",
    "2. Implement, train and visualize an auto-encoder architecture.\n",
    "3. Extend your implementation to a variational auto-encoder.\n",
    "4. Learn how to tune the critical beta parameter of your VAE.\n",
    "5. Inspect the learned representation of your VAE.\n",
    "\n",
    "\n",
    "**Note**: For faster training of the models in this assignment you can use Colab with enabled GPU support. In Colab, navigate to \"Runtime\" --> \"Change Runtime Type\" and set the \"Hardware Accelerator\" to \"GPU\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53lY_6Giu_7o"
   },
   "source": [
    "# 1. MNIST Dataset\n",
    "\n",
    "We will perform all experiments for this problem using the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), a standard dataset of handwritten digits. The main benefits of this dataset are that it is small and relatively easy to model. It therefore allows for quick experimentation and serves as initial test bed in many papers.\n",
    "\n",
    "Another benefit is that it is so widely used that PyTorch even provides functionality to automatically download it.\n",
    "\n",
    "Let's start by downloading the data and visualizing some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEq5AXeVlQSC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420,
     "referenced_widgets": [
      "420e71415e954b3db0e70bd284876136",
      "4a20e044157d409d8d5e67988bcdad0a",
      "3ca155d6f9f048a58cf23e5b37f9619f",
      "8c9f8f618db84976accadf0f9d2cb2a3",
      "d97a040b75ce44e6a270adc603e8828c",
      "6280ad2c259344ba83ad26c7c0ca97ea",
      "1b385db8e80144dd9b86c77325869ac9",
      "46ec4c380cc547fc9678860fbed3d1b0",
      "2d58f56a9d9446f0976ad268d9b93496",
      "891e36e3e4b44c4da077c405f0ad9f12",
      "5289bcbb51984fd0a4642e431fc23d85",
      "c7efd69ff7404880b3e1e6817bd67419",
      "2e2c7bcc35594556ae12afe35ea5d0a6",
      "dfc9b0cc298c4cfe9784a5fe38d94f0a",
      "b78d5ac826044365b60310288d7f73ae",
      "a3b65efa30014480b659455af8406120",
      "10a7a4fcb8be4079a86b7466fe45c598",
      "556456ef13524942a01869eb32c1f5ca",
      "37dc09e61b9844cab26935ac3454e895",
      "bf0d61993f7142f8a6c8870f93c1af91",
      "01763abed57f42b9b2c9e102d00d42a9",
      "0294efed745b4bb2b569069f818a28ea",
      "2ff5d6475dea46cd806e474616614bad",
      "bf05247b33b947baadb1601cbbbc8742",
      "4959520f1da249a292e3c0c7572e196c",
      "3081699983124f5b9c539dfb048bb4e9",
      "0bdf0114b0d24b63b7b0b65c29c572bd",
      "780b52dd827041d281d1379d3995041d",
      "92786db085a6430d8bc0d16ef91242ca",
      "9ac53ce81f51456eb53e6ea817cdf309",
      "3cdcf60b93cf4dcc9b24c2c2de8bd341",
      "a90264dd7b3649e3b8f314d3ec337ff2"
     ]
    },
    "id": "Rdkn5v6WvNAa",
    "outputId": "a5507e3c-b659-41e3-9388-95657f500c35"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# this will automatically download the MNIST training set\n",
    "mnist_train = torchvision.datasets.MNIST(root='./data', \n",
    "                                         train=True, \n",
    "                                         download=True, \n",
    "                                         transform=torchvision.transforms.ToTensor())\n",
    "print(\"\\n Download complete! Downloaded {} training examples!\".format(len(mnist_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "gVr1sOuWwk_F",
    "outputId": "9c06e5c9-4a55-44d9-b97f-cbe274b83468"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Let's display some of the training samples.\n",
    "sample_images = []\n",
    "mnist_it = iter(mnist_train)  # create simple iterator, later we will use proper DataLoader\n",
    "for _ in range(5):\n",
    "  sample = next(mnist_it)     # samples a tuple (image, label)\n",
    "  sample_images.append(sample[0][0].data.cpu().numpy())\n",
    "\n",
    "fig = plt.figure(figsize = (10, 50))   \n",
    "ax1 = plt.subplot(111)\n",
    "ax1.imshow(np.concatenate(sample_images, axis=1), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suPd0gyiuvvo"
   },
   "source": [
    "# 2. Auto-Encoder\n",
    "\n",
    "Before implementing the full VAE, we will first implement an **auto-encoder architecture**. Auto-encoders feature the same encoder-decoder architecture as VAEs and therefore also learn a low-dimensional representation of the input data without supervision. In contrast to VAEs they are **fully deterministic** models and do not employ variational inference for optimization.\n",
    "\n",
    "The **architecture** is very simple: we will encode the input image into a low-dimensional representation using a convolutional network with strided convolutions that reduce the image resolution in every layer. This results in a low-dimensional representation of the input image. This representation will get decoded back into the dimensionality of the input image using a convolutional decoder network that mirrors the architecture of the encoder. It employs transposed convolutions to increase the resolution of its input in every layer. The whole model is trained by **minimizing a reconstruction loss** between the input and the decoded image.\n",
    "\n",
    "Intuitively, the **auto-encoder needs to compress the information contained in the input image** into a much lower dimensional representation (e.g. 28x28=784px vs. 64 embedding dimensions for our MNIST model). This is possible since the information captured in the pixels is *highly redundant*. E.g. encoding an MNIST image requires <4 bits to encode which of the 10 possible digits is displayed and a few additional bits to capture information about shape and orientation. This is much less than the $255^{28\\cdot 28}$ bits of information that could be theoretically captured in the input image.\n",
    "\n",
    "Learning such a **compressed representation can make downstream task learning easier**. For example, learning to add two numbers based on the inferred digits is much easier than performing the task based on two piles of pixel values that depict the digits.\n",
    "\n",
    "In the following, we will first define the architecture of encoder and decoder and then train the auto-encoder model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akSPKMJ0l3rD"
   },
   "source": [
    "## Defining the Auto-Encoder Architecture [6pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rteiFTqfuvTu"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Let's define encoder and decoder networks\n",
    "#####################################################################\n",
    "# Encoder Architecture:                                             #\n",
    "#   - Conv2d, hidden units: 32, output resolution: 14x14, kernel: 4 #\n",
    "#   - LeakyReLU                                                     #\n",
    "#   - Conv2d, hidden units: 64, output resolution: 7x7, kernel: 4   #\n",
    "#   - BatchNorm2d                                                   #\n",
    "#   - LeakyReLU                                                     #\n",
    "#   - Conv2d, hidden units: 128, output resolution: 3x3, kernel: 3  #\n",
    "#   - BatchNorm2d                                                   #\n",
    "#   - LeakyReLU                                                     #\n",
    "#   - Conv2d, hidden units: 256, output resolution: 1x1, kernel: 3  #\n",
    "#   - BatchNorm2d                                                   #\n",
    "#   - LeakyReLU                                                     #\n",
    "#   - Flatten                                                       #\n",
    "#   - Linear, output units: nz (= representation dimensionality)    #\n",
    "#####################################################################\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, nz):\n",
    "    super().__init__()\n",
    "    ################################# TODO #########################################\n",
    "    # Create the network architecture using a nn.Sequential module wrapper.        #\n",
    "    # All convolutional layers should also learn a bias.                           #\n",
    "    # HINT: use the given information to compute stride and padding                #\n",
    "    #       for each convolutional layer. Verify the shapes of intermediate layers #\n",
    "    #       by running partial networks (with the next cell) and visualizing the   #\n",
    "    #       output shapes.                                                         #\n",
    "    ################################################################################\n",
    "    self.net = nn.Sequential(\n",
    "        # add your network layers here\n",
    "        # ...\n",
    "    )\n",
    "    ################################ END TODO #######################################\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.net(x)\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "# Decoder Architecture (mirrors encoder architecture):              #\n",
    "#   - Linear, output units: 256                                     #\n",
    "#   - Reshape, output shape: (256, 1, 1)                            #\n",
    "#   - BatchNorm2d                                                   #\n",
    "#   - LeakyReLU                                                     #\n",
    "#   - ConvT2d, hidden units: 128, output resolution: 3x3, kernel: 3 #\n",
    "#   - BatchNorm2d                                                   #\n",
    "#   - LeakyReLU                                                     #\n",
    "#   - ConvT2d, hidden units: 64, output resolution: 7x7, kernel: 3  #\n",
    "#   - ...                                                           #\n",
    "#   - ...                                                           #\n",
    "#   - ConvT2d, output units: 1, output resolution: 28x28, kernel: 4 #\n",
    "#   - Sigmoid (to limit output in range [0...1])                    #\n",
    "#####################################################################\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, nz):\n",
    "    super().__init__()\n",
    "    ################################# TODO #########################################\n",
    "    # Create the network architecture using a nn.Sequential module wrapper.        #\n",
    "    # Again, all (transposed) convolutional layers should also learn a bias.       #\n",
    "    # We need to separate the intial linear layer into a separate variable since   #\n",
    "    # nn.Sequential does not support reshaping. Instead the \"Reshape\" is performed #\n",
    "    # in the forward() function below and does not need to be added to self.net    #\n",
    "    # HINT: use the class nn.ConvTranspose2d for the transposed convolutions.      #\n",
    "    #       Verify the shapes of intermediate layers by running partial networks   #\n",
    "    #       (using the next cell) and visualizing the output shapes.               #\n",
    "    ################################################################################\n",
    "    self.map = None   # for initial Linear layer\n",
    "    self.net = nn.Sequential(\n",
    "        # add your network layers here\n",
    "        # ...\n",
    "    )\n",
    "    ################################ END TODO #######################################\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.net(self.map(x).reshape(-1, 256, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0PKgbP4l-n8"
   },
   "source": [
    "## Testing the Auto-Encoder Forward Pass [1pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "-m9ur743t-22",
    "outputId": "bc4b9ee5-e894-4424-c1fc-07700b3e450a"
   },
   "outputs": [],
   "source": [
    "# To test your encoder/decoder, let's encode/decode some sample images\n",
    "# first, make a PyTorch DataLoader object to sample data batches\n",
    "batch_size = 64\n",
    "nworkers = 4        # number of wrokers used for efficient data loading\n",
    "\n",
    "####################################### TODO #######################################\n",
    "# Create a PyTorch DataLoader object for efficiently generating training batches.  #\n",
    "# Make sure that the data loader automatically shuffles the training dataset.      #\n",
    "# HINT: The DataLoader wraps the MNIST dataset class we created earlier.           #\n",
    "#       Use the given batch_size and number of data loading workers when creating  #\n",
    "#       the DataLoader.                                                            #\n",
    "####################################################################################\n",
    "mnist_data_loader = None\n",
    "#################################### END TODO #######################################\n",
    "\n",
    "# now we can run a forward pass for encoder and decoder and check the produced shapes\n",
    "nz = 64          # dimensionality of the learned embedding\n",
    "encoder = Encoder(nz)\n",
    "decoder = Decoder(nz)\n",
    "for sample_img, sample_label in mnist_data_loader:\n",
    "  enc = encoder(sample_img)\n",
    "  print(\"Shape of encoding vector (should be [batch_size, nz]): {}\".format(enc.shape))\n",
    "  dec = decoder(enc)\n",
    "  print(\"Shape of decoded image (should be [batch_size, 1, 28, 28]): {}\".format(dec.shape))    \n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ssk-NnNE9vO"
   },
   "source": [
    "Now that we defined encoder and decoder network our architecture is nearly complete. However, before we start training, we can wrap encoder and decoder into an auto-encoder class for easier handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LziabE_5E9IG"
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "  def __init__(self, nz):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(nz)\n",
    "    self.decoder = Decoder(nz)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.decoder(self.encoder(x))\n",
    "\n",
    "  def reconstruct(self, x):\n",
    "    \"\"\"Only used later for visualization.\"\"\"\n",
    "    return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wo2rQhAVEYp3"
   },
   "source": [
    "## Setting up the Auto-Encoder Training Loop [6pt]\n",
    "After implementing the network architecture, we can now set up the training loop and run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7vWaTyoqEnWK",
    "outputId": "50e1e2e0-211a-4f5d-febe-283459651747"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# build AE model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')   # use GPU if available\n",
    "ae_model = AutoEncoder(nz).to(device)    # transfer model to GPU if available\n",
    "ae_model = ae_model.train()   # set model in train mode (eg batchnorm params get updated)\n",
    "\n",
    "# build optimizer and loss function\n",
    "####################################### TODO #######################################\n",
    "# Create the optimizer and loss classes. For the loss you can use a loss layer     #\n",
    "# from the torch.nn package.                                                       #\n",
    "# HINT: We will use the Adam optimizer (learning rate given above, otherwise       #\n",
    "#       default parameters) and MSE loss for the criterion / loss.                 #\n",
    "# NOTE: We could also use alternative loss functions like cross entropy, depending #\n",
    "#       on the assumptions we are making about the output distribution. Here we    #\n",
    "#       will use MSE loss as it is the most common choice, assuming a Gaussian     #\n",
    "#       output distribution.                                                       #\n",
    "####################################################################################\n",
    "opt = None          # create optimizer instance\n",
    "criterion = None    # create loss layer instance\n",
    "#################################### END TODO #######################################\n",
    "\n",
    "train_it = 0\n",
    "for ep in range(epochs):\n",
    "  print(\"Run Epoch {}\".format(ep))\n",
    "  ####################################### TODO #######################################\n",
    "  # Implement the main training loop for the auto-encoder model.                     #\n",
    "  # HINT: Your training loop should sample batches from the data loader, run the     #\n",
    "  #       forward pass of the AE, compute the loss, perform the backward pass and    #\n",
    "  #       perform one gradient step with the optimizer.                              #\n",
    "  # HINT: Don't forget to erase old gradients before performing the backward pass.   #\n",
    "  ####################################################################################\n",
    "  for None in None:\n",
    "    # add training loop commands here\n",
    "    # ...\n",
    "    #################################### END TODO #####################################\n",
    "\n",
    "    if train_it % 100 == 0:\n",
    "      print(\"It {}: Reconstruction Loss: {}\".format(train_it, rec_loss))\n",
    "    train_it += 1\n",
    "  \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3blvOcuUnKpp"
   },
   "source": [
    "## Verifying reconstructions [0pt]\n",
    "Now that we trained the auto-encoder we can visualize some of the reconstructions on the test set to verify that it is converged and did not overfit. **Before continuing, make sure that your auto-encoder is able to reconstruct these samples near-perfectly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "HmbdWzLxHuXV",
    "outputId": "f6d30ecb-65fc-41f0-9f9d-9ed4e72eaab8"
   },
   "outputs": [],
   "source": [
    "# visualize test data reconstructions\n",
    "def vis_reconstruction(model):\n",
    "  # download MNIST test set + build Dataset object\n",
    "  mnist_test = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          download=True, \n",
    "                                          transform=torchvision.transforms.ToTensor())\n",
    "  mnist_test_iter = iter(mnist_test)\n",
    "  model.eval()      # set model in evalidation mode (eg freeze batchnorm params)\n",
    "  input_imgs, test_reconstructions = [], []\n",
    "  for _ in range(5):\n",
    "    input_img = np.asarray(next(mnist_test_iter)[0])\n",
    "    reconstruction = model.reconstruct(torch.tensor(input_img[None], device=device))\n",
    "    input_imgs.append(input_img[0])\n",
    "    test_reconstructions.append(reconstruction[0, 0].data.cpu().numpy())\n",
    "\n",
    "  fig = plt.figure(figsize = (20, 50))   \n",
    "  ax1 = plt.subplot(111)\n",
    "  ax1.imshow(np.concatenate([np.concatenate(input_imgs, axis=1),\n",
    "                            np.concatenate(test_reconstructions, axis=1)], axis=0), cmap='gray')\n",
    "  plt.show()\n",
    "\n",
    "vis_reconstruction(ae_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7UO6153nWmC"
   },
   "source": [
    "## Sampling from the Auto-Encoder [2pt]\n",
    "\n",
    "To test whether the auto-encoder is useful as a generative model, we can use it like any other generative model: draw embedding samples from a prior distribution and decode them through the decoder network. We will choose a unit Gaussian prior to allow for easy comparison to the VAE later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "id": "tsWgAMfEn3Qk",
    "outputId": "cfb91e85-f5d2-4caa-8ee2-1855b399f83c"
   },
   "outputs": [],
   "source": [
    "# we will sample N embeddings, then decode and visualize them\n",
    "def vis_samples(model):\n",
    "  ####################################### TODO #######################################\n",
    "  # Sample embeddings from a diagonal unit Gaussian distribution and decode them     #\n",
    "  # using the model.                                                                 #\n",
    "  # HINT: The sampled embeddings should have shape [batch_size, nz]. Diagonal unit   #\n",
    "  #       Gaussians have mean 0 and a covariance matrix with ones on the diagonal    #\n",
    "  #       and zeros everywhere else.                                                 #\n",
    "  # HINT: If you are unsure whether you sampled the correct distribution, you can    #\n",
    "  #       sample a large batch and compute the empirical mean and variance using the #\n",
    "  #       .mean() and .var() functions.                                              #\n",
    "  # HINT: You can directly use model.decoder() to decode the samples.                #\n",
    "  ####################################################################################\n",
    "  sampled_embeddings = None    # sample batch of embedding from prior\n",
    "  decoded_samples = None       # decoder output images for sampled embeddings\n",
    "  #################################### END TODO ######################################\n",
    "\n",
    "  fig = plt.figure(figsize = (10, 10))   \n",
    "  ax1 = plt.subplot(111)\n",
    "  ax1.imshow(torchvision.utils.make_grid(decoded_samples[:16], nrow=4, pad_value=1.)\\\n",
    "                .data.cpu().numpy().transpose(1, 2, 0), cmap='gray')\n",
    "  plt.show()\n",
    "\n",
    "vis_samples(ae_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExC5BXn3rbap"
   },
   "source": [
    ">**Inline Question: Describe your observations, why do you think they occur? [2pt]** \\\\\n",
    ">(please limit your answer to <150 words) \\\\\n",
    ">**Answer:** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSUezleArhCI"
   },
   "source": [
    "# 3. Variational Auto-Encoder (VAE)\n",
    "\n",
    "Variational auto-encoders use a very similar architecture to deterministic auto-encoders, but are inherently storchastic models, i.e. we perform a stochastic sampling operation during the forward pass, leading to different different outputs every time we run the network for the same input. This sampling is required to optimize the VAE objective also known as the evidence lower bound (ELBO):\n",
    "\n",
    "$$\n",
    "p(x) > \\underbrace{\\mathbb{E}_{z\\sim q(z\\vert x)} p(x \\vert z)}_{\\text{reconstruction}} - \\underbrace{D_{\\text{KL}}\\big(q(z \\vert x), p(z)\\big)}_{\\text{prior divergence}}\n",
    "$$\n",
    "\n",
    "Here, $D_{\\text{KL}}(q, p)$ denotes the Kullback-Leibler (KL) divergence between the posterior distribution $q(z \\vert x)$, i.e. the output of our encoder, and $p(z)$, the prior over the embedding variable $z$, which we can choose freely.\n",
    "\n",
    "For simplicity, we will again choose a unit Gaussian prior. The left term is the reconstruction term we already know from training the auto-encoder. When assuming a Gaussian output distribution for both encoder $q(z \\vert x)$ and decoder $p(x \\vert z)$ the objective reduces to:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{VAE}} = \\sum_{x\\sim \\mathcal{D}} (x - \\hat{x})^2 - \\beta \\cdot D_{\\text{KL}}\\big(\\mathcal{N}(\\mu_q, \\sigma_q), \\mathcal{N}(0, I)\\big)\n",
    "$$\n",
    "\n",
    "Here, $\\hat{x}$ is the reconstruction output of the decoder. In comparison to the auto-encoder objetive, the VAE adds a regularizing term between the output of the encoder and a chosen prior distribution, effectively forcing the encoder output to not stray too far from the prior during training. As a result the decoder gets trained with samples that look pretty similar to samples from the prior, which will hopefully allow us to generate better images when using the VAE as a generative model and actually feeding it samples from the prior (as we have done for the AE before).\n",
    "\n",
    "The coefficient $\\beta$ is a scalar weighting factor that trades off between reconstruction and regularization objective. We will investigate the influence of this factor in out experiments below.\n",
    "\n",
    "If you need a refresher on VAEs you can check out this tutorial paper: https://arxiv.org/abs/1606.05908\n",
    "\n",
    "### Reparametrization Trick\n",
    "\n",
    "The sampling procedure inside the VAE's forward pass for obtaining a sample $z$ from the posterior distribution $q(z \\vert x)$, when implemented naively, is non-differentiable. However, since $q(z\\vert x)$ is parametrized with a Gaussian function, there is a simple trick to obtain a differentiable sampling operator, known as the _reparametrization trick_.\n",
    "\n",
    "Instead of directly sampling $z \\sim \\mathcal{N}(\\mu_q, \\sigma_q)$ we can \"separate\" the network's predictions and the random sampling by computing the sample as:\n",
    "\n",
    "$$\n",
    "z = \\mu_q + \\sigma_q * \\epsilon , \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "Note that in this equation, the sample $z$ is computed as a deterministic function of the network's predictions $\\mu_q$ and $\\sigma_q$ and therefore allows to propagate gradients through the sampling procedure.\n",
    "\n",
    "**Note**: While in the equations above the encoder network parametrizes the standard deviation $\\sigma_q$ of the Gaussian posterior distribution, in practice we usually parametrize the **logarithm of the standard deviation** $\\log \\sigma_q$ for numerical stability. Before sampling $z$ we will then exponentiate the network's output to obtain $\\sigma_q$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSC7_Yy-n6-G"
   },
   "source": [
    "## Defining the VAE Model [7pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qofk8oQ9tAxd"
   },
   "outputs": [],
   "source": [
    "def kl_divergence(mu1, log_sigma1, mu2, log_sigma2):\n",
    "  \"\"\"Computes KL[p||q] between two Gaussians defined by [mu, log_sigma].\"\"\"\n",
    "  return (log_sigma2 - log_sigma1) + (torch.exp(log_sigma1) ** 2 + (mu1 - mu2) ** 2) \\\n",
    "               / (2 * torch.exp(log_sigma2) ** 2) - 0.5\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "  def __init__(self, nz, beta=1.0):\n",
    "    super().__init__()\n",
    "    self.beta = beta          # factor trading off between two loss components\n",
    "    ####################################### TODO #######################################\n",
    "    # Instantiate Encoder and Decoder.                                                 #\n",
    "    # HINT: Remember that the encoder is now parametrizing a Gaussian distribution's   #\n",
    "    #       mean and log_sigma, so the dimensionality of the output embedding needs to #\n",
    "    #       double.                                                                    #\n",
    "    ####################################################################################\n",
    "    self.encoder = None\n",
    "    self.decoder = None\n",
    "    #################################### END TODO ######################################\n",
    "\n",
    "  def forward(self, x):\n",
    "    ####################################### TODO #######################################\n",
    "    # Implement the forward pass of the VAE.                                           #\n",
    "    # HINT: Your code should implement the following steps:                            #\n",
    "    #          1. encode input x, split encoding into mean and log_sigma of Gaussian   #\n",
    "    #          2. sample z from inferred posterior distribution using                  #\n",
    "    #             reparametrization trick                                              #\n",
    "    #          3. decode the sampled z to obtain the reconstructed image               #\n",
    "    ####################################################################################\n",
    "    # encode input into posterior distribution q(z | x)\n",
    "    q = None       # output of encoder (concatenated mean and log_sigma)\n",
    "\n",
    "    # sample latent variable z with reparametrization\n",
    "    z = None       # batch of sampled embeddings\n",
    "\n",
    "    # compute reconstruction\n",
    "    reconstruction = None   # decoder reconstruction from embedding\n",
    "    #################################### END TODO ######################################\n",
    "\n",
    "    return {'q': q, \n",
    "            'rec': reconstruction}\n",
    "\n",
    "  def loss(self, x, outputs):\n",
    "    ####################################### TODO #######################################\n",
    "    # Implement the loss computation of the VAE.                                       #\n",
    "    # HINT: Your code should implement the following steps:                            #\n",
    "    #          1. compute the image reconstruction loss, similar to AE we use MSE loss #\n",
    "    #          2. compute the KL divergence loss between the inferred posterior        #\n",
    "    #             distribution and a unit Gaussian prior; you can use the provided     #\n",
    "    #             function above for computing the KL divergence between two Gaussians #\n",
    "    #             parametrized by mean and log_sigma                                   #\n",
    "    # HINT: Make sure to compute the KL divergence in the correct order since it is    #\n",
    "    #       not symmetric, ie. KL(p, q) != KL(q, p)!                                   #\n",
    "    ####################################################################################\n",
    "    # compute reconstruction loss\n",
    "    rec_loss = None\n",
    "\n",
    "    # compute KL divergence loss\n",
    "    kl_loss = None      # make sure that this is a scalar, not a vector / array\n",
    "    #################################### END TODO ######################################\n",
    "\n",
    "    # return weihgted objective\n",
    "    return rec_loss + self.beta * kl_loss, \\\n",
    "           {'rec_loss': rec_loss, 'kl_loss': kl_loss}\n",
    "    \n",
    "  def reconstruct(self, x):\n",
    "    \"\"\"Use mean of posterior estimate for visualization reconstruction.\"\"\"\n",
    "    ####################################### TODO #######################################\n",
    "    # This function is used for visualizing reconstructions of our VAE model. To       #\n",
    "    # obtain the maximum likelihood estimate we bypass the sampling procedure of the   #\n",
    "    # inferred latent and instead directly use the mean of the inferred posterior.     #\n",
    "    # HINT: encode the input image and then decode the mean of the posterior to obtain #\n",
    "    #       the reconstruction.                                                        #\n",
    "    ####################################################################################\n",
    "    reconstruction = None\n",
    "    #################################### END TODO ######################################\n",
    "    return reconstruction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPCQZr-s_INx"
   },
   "source": [
    "## Setting up the VAE Training Loop [4pt]\n",
    "\n",
    "Let's start training the VAE model! We will first verify our implementation by setting $\\beta = 0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2StBTjj__HBU",
    "outputId": "01433650-b335-4dcd-b9d5-1c960e1fca72"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "nz = 64\n",
    "\n",
    "####################################### TODO #######################################\n",
    "# Tune the beta parameter to obtain good VAE training results. However, for the    #\n",
    "# initial experiments leave beta = 0 in order to verify our implementation.        #\n",
    "####################################################################################\n",
    "epochs = 5         # using 5 epochs is sufficient for the first two experiments\n",
    "                   # for the experiment where you tune beta, 20 epochs are appropriate\n",
    "beta = 0.0\n",
    "#################################### END TODO ######################################\n",
    "\n",
    "# build VAE model\n",
    "vae_model = VAE(nz, beta).to(device)    # transfer model to GPU if available\n",
    "vae_model = vae_model.train()   # set model in train mode (eg batchnorm params get updated)\n",
    "\n",
    "# build optimizer and loss function\n",
    "####################################### TODO #######################################\n",
    "# Build the optimizer for the vae_model. We will again use the Adam optimizer with #\n",
    "# the given learning rate and otherwise default parameters.                        #\n",
    "####################################################################################\n",
    "opt = None\n",
    "#################################### END TODO ######################################\n",
    "\n",
    "train_it = 0\n",
    "rec_loss, kl_loss = [], []\n",
    "for ep in range(epochs):\n",
    "  print(\"Run Epoch {}\".format(ep))\n",
    "  ####################################### TODO #######################################\n",
    "  # Implement the main training loop for the VAE model.                              #\n",
    "  # HINT: Your training loop should sample batches from the data loader, run the     #\n",
    "  #       forward pass of the VAE, compute the loss, perform the backward pass and   #\n",
    "  #       perform one gradient step with the optimizer.                              #\n",
    "  # HINT: Don't forget to erase old gradients before performing the backward pass.   #\n",
    "  # HINT: This time we will use the loss() function of our model for computing the   #\n",
    "  #       training loss. It outputs the total training loss and a dict containing    #\n",
    "  #       the breakdown of reconstruction and KL loss.                               #\n",
    "  ####################################################################################\n",
    "  for None in None:\n",
    "    # add VAE training loop commands here\n",
    "    # ...\n",
    "    #################################### END TODO ####################################\n",
    "\n",
    "    rec_loss.append(losses['rec_loss']); kl_loss.append(losses['kl_loss'])\n",
    "    if train_it % 100 == 0:\n",
    "      print(\"It {}: Total Loss: {}, \\t Rec Loss: {},\\t KL Loss: {}\"\\\n",
    "            .format(train_it, total_loss, losses['rec_loss'], losses['kl_loss']))\n",
    "    train_it += 1\n",
    "  \n",
    "print(\"Done!\")\n",
    "\n",
    "# log the loss training curves\n",
    "fig = plt.figure(figsize = (10, 5))   \n",
    "ax1 = plt.subplot(121)\n",
    "ax1.plot(rec_loss)\n",
    "ax1.title.set_text(\"Reconstruction Loss\")\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.plot(kl_loss)\n",
    "ax2.title.set_text(\"KL Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDK4A6WQPgUi"
   },
   "source": [
    "Let's look at some reconstructions and decoded embedding samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2KnFxjABPmQP",
    "outputId": "d83d022b-4e38-4bbd-9adb-28f9d01a83c1"
   },
   "outputs": [],
   "source": [
    "# visualize VAE reconstructions and samples from the generative model\n",
    "vis_reconstruction(vae_model)\n",
    "vis_samples(vae_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlofqS_pUcOU"
   },
   "source": [
    ">**Inline Question: What can you observe when setting $\\beta = 0$? Explain your observations! [3pt]** \\\\\n",
    ">(please limit your answer to <150 words) \\\\\n",
    ">**Answer:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqFtLvsIEwRy"
   },
   "source": [
    "Let's repeat the same experiment for $\\beta = 10$, a very high value for the coefficient. You can modify the $\\beta$ value in the cell above and rerun it (it is okay to overwrite the outputs of the previous experiment, but **make sure to copy the visualizations of training curves, reconstructions and samples for $\\beta = 0$ into your solution PDF** before deleting them).\n",
    "\n",
    ">**Inline Question: What can you observe when setting $\\beta = 10$? Explain your observations! [3pt]** \\\\\n",
    ">(please limit your answer to <200 words) \\\\\n",
    ">**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_pdzfj--tUB"
   },
   "source": [
    "Now we can start tuning the beta value to achieve a good result. First describe what a \"good result\" would look like (focus what you would expect for reconstructions and sample quality). \n",
    "\n",
    ">**Inline Question: Characterize what properties you would expect for reconstructions (1pt) and samples (2pt) of a well-tuned VAE! [3pt]** \\\\\n",
    ">(please limit your answer to <200 words) \\\\\n",
    ">**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpznmVGvGXjO"
   },
   "source": [
    "## Tuning the $\\beta$-factor [5pt]\n",
    "Now that you know what outcome we would like to obtain, try to tune $\\beta$ to achieve this result. \n",
    "\n",
    "(logarithmic search in steps of 10x will be helpful, good results can be achieved after ~20 epochs of training). It is again okay to overwrite the results of the previous $\\beta=10$ experiment after copying them to the solution PDF. \n",
    "\n",
    "**Your final notebook should include the visualizations of your best-tuned VAE.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtU4-fHnAbYL"
   },
   "source": [
    "# 4. Embedding Space Interpolation [3pt]\n",
    "\n",
    "As mentioned in the introduction, AEs and VAEs cannot only be used to generate images, but also to learn low-dimensional representations of their inputs. In this final section we will investigate the representations we learned with both models by **interpolating in embedding space** between different images. We will encode two images into their low-dimensional embedding representations, then interpolate these embeddings and reconstruct the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "9VO_Dj-nAw6z",
    "outputId": "3b2fe02c-c462-4fd5-a063-a0da1ec53595"
   },
   "outputs": [],
   "source": [
    "START_LABEL = 5\n",
    "END_LABEL = 8\n",
    "nz=64\n",
    "\n",
    "def get_image_with_label(target_label):\n",
    "  \"\"\"Returns a random image from the training set with the requested digit.\"\"\"\n",
    "  for img_batch, label_batch in mnist_data_loader:\n",
    "    for img, label in zip(img_batch, label_batch):\n",
    "      if label == target_label:\n",
    "        return img.to(device)\n",
    "\n",
    "def interpolate_and_visualize(model, tag, start_img, end_img):\n",
    "  \"\"\"Encodes images and performs interpolation. Displays decodings.\"\"\"\n",
    "  model.eval()    # put model in eval mode to avoid updating batchnorm\n",
    "\n",
    "  # encode both images into embeddings (use posterior mean for interpolation)\n",
    "  z_start = model.encoder(start_img[None])[..., :nz]\n",
    "  z_end = model.encoder(end_img[None])[..., :nz]\n",
    "\n",
    "  # compute interpolated latents\n",
    "  N_INTER_STEPS = 5\n",
    "  z_inter = [z_start + i/N_INTER_STEPS * (z_end - z_start) for i in range(N_INTER_STEPS)]\n",
    "\n",
    "  # decode interpolated embeddings (as a single batch)\n",
    "  img_inter = model.decoder(torch.cat(z_inter))\n",
    "\n",
    "  # reshape result and display interpolation\n",
    "  vis_imgs = torch.cat([start_img[None], img_inter, end_img[None]])\n",
    "  fig = plt.figure(figsize = (10, 10))   \n",
    "  ax1 = plt.subplot(111)\n",
    "  ax1.imshow(torchvision.utils.make_grid(vis_imgs, nrow=N_INTER_STEPS+2, pad_value=1.)\\\n",
    "                  .data.cpu().numpy().transpose(1, 2, 0), cmap='gray')\n",
    "  plt.title(tag)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "# sample two training images with given labels\n",
    "start_img = get_image_with_label(START_LABEL)\n",
    "end_img = get_image_with_label(END_LABEL)\n",
    "\n",
    "# visualize interpolations for AE and VAE models\n",
    "interpolate_and_visualize(ae_model, \"Auto-Encoder\", start_img, end_img)\n",
    "interpolate_and_visualize(vae_model, \"Variational Auto-Encoder\", start_img, end_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdQKHspOF_5Q"
   },
   "source": [
    "Repeat the experiment for different start / end labels and different samples. Describe your observations.\n",
    "\n",
    ">**Inline Question: Repeat the interpolation experiment with different start / end labels and multiple samples. Describe your observations!\n",
    "Focus on**: \\\\\n",
    "  1. **How do AE and VAE embedding space interpolations differ?** \\\\\n",
    "  2. **How do you expect these differences to affect the usefulness of the learned representation for downstream learning?** \\\\\n",
    "(please limit your answer to <300 words)\n",
    "\n",
    ">**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytPPsGz2vBx0"
   },
   "source": [
    "# Submission PDF\n",
    "\n",
    "As in assignment 1, please prepare a separate submission PDF for each problem. For this problem, please include the following plots & answers in a PDF called `problem_1_solution.pdf`:\n",
    "\n",
    "1. Auto-encoder samples and AE sampling inline question answer.\n",
    "2. VAE training curves, reconstructions and samples for:\n",
    "  * $\\beta = 0$\n",
    "  * $\\beta = 10$\n",
    "  * your tuned $\\beta$ (also listing the tuned value for $\\beta$)\n",
    "3. Answers to all inline questions in VAE section (ie 4 inline questions).\n",
    "4. Three representative interpolation comparisons that show AE and VAE embedding interpolation between the same images.\n",
    "5. Answer to interpolation inline question.\n",
    "\n",
    "Note that you still need to submit the jupyter notebook with all generated solutions. We will randomly pick submissions and check that the plots in the PDF and in the notebook are equivalent (except for those $\\beta=0\\;/\\;10$ plots that we allowed to overwrite)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CSCI566_Assignment2_VAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01763abed57f42b9b2c9e102d00d42a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0294efed745b4bb2b569069f818a28ea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0bdf0114b0d24b63b7b0b65c29c572bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ac53ce81f51456eb53e6ea817cdf309",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_92786db085a6430d8bc0d16ef91242ca",
      "value": 1
     }
    },
    "10a7a4fcb8be4079a86b7466fe45c598": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_37dc09e61b9844cab26935ac3454e895",
       "IPY_MODEL_bf0d61993f7142f8a6c8870f93c1af91"
      ],
      "layout": "IPY_MODEL_556456ef13524942a01869eb32c1f5ca"
     }
    },
    "1b385db8e80144dd9b86c77325869ac9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2d58f56a9d9446f0976ad268d9b93496": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5289bcbb51984fd0a4642e431fc23d85",
       "IPY_MODEL_c7efd69ff7404880b3e1e6817bd67419"
      ],
      "layout": "IPY_MODEL_891e36e3e4b44c4da077c405f0ad9f12"
     }
    },
    "2e2c7bcc35594556ae12afe35ea5d0a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2ff5d6475dea46cd806e474616614bad": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3081699983124f5b9c539dfb048bb4e9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37dc09e61b9844cab26935ac3454e895": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0294efed745b4bb2b569069f818a28ea",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_01763abed57f42b9b2c9e102d00d42a9",
      "value": 1
     }
    },
    "3ca155d6f9f048a58cf23e5b37f9619f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6280ad2c259344ba83ad26c7c0ca97ea",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d97a040b75ce44e6a270adc603e8828c",
      "value": 1
     }
    },
    "3cdcf60b93cf4dcc9b24c2c2de8bd341": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "420e71415e954b3db0e70bd284876136": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3ca155d6f9f048a58cf23e5b37f9619f",
       "IPY_MODEL_8c9f8f618db84976accadf0f9d2cb2a3"
      ],
      "layout": "IPY_MODEL_4a20e044157d409d8d5e67988bcdad0a"
     }
    },
    "46ec4c380cc547fc9678860fbed3d1b0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4959520f1da249a292e3c0c7572e196c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0bdf0114b0d24b63b7b0b65c29c572bd",
       "IPY_MODEL_780b52dd827041d281d1379d3995041d"
      ],
      "layout": "IPY_MODEL_3081699983124f5b9c539dfb048bb4e9"
     }
    },
    "4a20e044157d409d8d5e67988bcdad0a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5289bcbb51984fd0a4642e431fc23d85": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dfc9b0cc298c4cfe9784a5fe38d94f0a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2e2c7bcc35594556ae12afe35ea5d0a6",
      "value": 1
     }
    },
    "556456ef13524942a01869eb32c1f5ca": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6280ad2c259344ba83ad26c7c0ca97ea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "780b52dd827041d281d1379d3995041d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a90264dd7b3649e3b8f314d3ec337ff2",
      "placeholder": "​",
      "style": "IPY_MODEL_3cdcf60b93cf4dcc9b24c2c2de8bd341",
      "value": " 8192/? [00:00&lt;00:00, 14736.38it/s]"
     }
    },
    "891e36e3e4b44c4da077c405f0ad9f12": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c9f8f618db84976accadf0f9d2cb2a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46ec4c380cc547fc9678860fbed3d1b0",
      "placeholder": "​",
      "style": "IPY_MODEL_1b385db8e80144dd9b86c77325869ac9",
      "value": " 9920512/? [00:03&lt;00:00, 3119162.02it/s]"
     }
    },
    "92786db085a6430d8bc0d16ef91242ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9ac53ce81f51456eb53e6ea817cdf309": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3b65efa30014480b659455af8406120": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a90264dd7b3649e3b8f314d3ec337ff2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b78d5ac826044365b60310288d7f73ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf05247b33b947baadb1601cbbbc8742": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf0d61993f7142f8a6c8870f93c1af91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf05247b33b947baadb1601cbbbc8742",
      "placeholder": "​",
      "style": "IPY_MODEL_2ff5d6475dea46cd806e474616614bad",
      "value": " 1654784/? [00:01&lt;00:00, 1192543.22it/s]"
     }
    },
    "c7efd69ff7404880b3e1e6817bd67419": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3b65efa30014480b659455af8406120",
      "placeholder": "​",
      "style": "IPY_MODEL_b78d5ac826044365b60310288d7f73ae",
      "value": " 32768/? [00:00&lt;00:00, 95097.27it/s]"
     }
    },
    "d97a040b75ce44e6a270adc603e8828c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "dfc9b0cc298c4cfe9784a5fe38d94f0a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
