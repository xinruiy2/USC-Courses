{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d27bc3abe00d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# coding: utf-8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msoftmax_cross_entropy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_momentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader_mnist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataSplit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "from utils import softmax_cross_entropy, add_momentum, data_loader_mnist, predict_label, DataSplit\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "# 1. One linear Neural Network layer with forward and backward steps\n",
    "\n",
    "### Modules ###\n",
    "\n",
    "    ########################################################################################\n",
    "    #   The following three modules (class) are what you need to complete  (check TODO)    #\n",
    "    ########################################################################################\n",
    "\n",
    "class linear_layer:\n",
    "\n",
    "    \"\"\"\n",
    "        The linear (affine/fully-connected) module.\n",
    "\n",
    "        It is built up with two arguments:\n",
    "        - input_D: the dimensionality of the input example/instance of the forward pass\n",
    "        - output_D: the dimensionality of the output example/instance of the forward pass\n",
    "\n",
    "        It has two learnable parameters:\n",
    "        - self.params['W']: the W matrix (numpy array) of shape input_D-by-output_D\n",
    "        - self.params['b']: the b vector (numpy array) of shape 1-by-output_D\n",
    "\n",
    "        It will record the partial derivatives of loss w.r.t. self.params['W'] and self.params['b'] in:\n",
    "        - self.gradient['W']: input_D-by-output_D numpy array\n",
    "        - self.gradient['b']: 1-by-output_D numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_D, output_D):\n",
    "\n",
    "        self.params = dict()\n",
    "\n",
    "        ###############################################################################################\n",
    "        # TODO: Use np.random.normal() with mean as 0 and standard deviation as 0.1\n",
    "        # W Shape (input_D, output_D), b shape (1, output_D)\n",
    "        ###############################################################################################\n",
    "        self.params['W'] = np.random.normal(loc = 0.0, scale = 0.1,size = (input_D, output_D))\n",
    "        self.params['b'] = np.random.normal(loc = 0.0, scale = 0.1,size = (1, output_D))\n",
    "\n",
    "\n",
    "        self.gradient = dict()\n",
    "\n",
    "        ###############################################################################################\n",
    "        # TODO: Initialize gradients with zeros\n",
    "        # Note: Shape of gradient is same as the respective variables\n",
    "        ###############################################################################################\n",
    "        self.gradient['W'] = np.zeros((input_D, output_D))\n",
    "        self.gradient['b'] = np.zeros((1,output_D))\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        \"\"\"\n",
    "            The forward pass of the linear (affine/fully-connected) module.\n",
    "\n",
    "            Input:\n",
    "            - X: A N-by-input_D numpy array, where each 'row' is an input example/instance (i.e., X[i], where                   i = 1,...,N).\n",
    "                The mini-batch size is N.\n",
    "\n",
    "            Return:\n",
    "            - forward_output: A N-by-output_D numpy array, where each 'row' is an output example/instance.\n",
    "        \"\"\"\n",
    "\n",
    "        ################################################################################\n",
    "        # TODO: Implement the linear forward pass. Store the result in forward_output  #\n",
    "        ################################################################################\n",
    "        forward_output = X @ self.params['W'] + self.params['b']\n",
    "        return forward_output\n",
    "\n",
    "    def backward(self, X, grad):\n",
    "\n",
    "        \"\"\"\n",
    "            The backward pass of the linear (affine/fully-connected) module.\n",
    "\n",
    "            Input:\n",
    "            - X: A N-by-input_D numpy array, the input to the forward pass.\n",
    "            - grad: A N-by-output_D numpy array, where each 'row' (say row i) is the partial derivatives of the mini-batch loss\n",
    "                 w.r.t. forward_output[i].\n",
    "\n",
    "            Operation:\n",
    "            - Compute the partial derivatives (gradients) of the mini-batch loss w.r.t. self.params['W'], self.params['b'].\n",
    "\n",
    "            Return:\n",
    "            - backward_output: A N-by-input_D numpy array, where each 'row' (say row i) is the partial derivatives of the mini-batch loss w.r.t. X[i].\n",
    "        \"\"\"\n",
    "\n",
    "        #################################################################################################\n",
    "        # TODO: Implement the backward pass (i.e., compute the following three terms)\n",
    "        # self.gradient['W'] = ? (input_D-by-output_D numpy array, the gradient of the mini-batch loss w.r.t. self.params['W'])\n",
    "        # self.gradient['b'] = ? (1-by-output_D numpy array, the gradient of the mini-batch loss w.r.t. self.params['b'])\n",
    "        # backward_output = ? (N-by-input_D numpy array, the gradient of the mini-batch loss w.r.t. X)\n",
    "        # only return backward_output, but need to compute self.gradient['W'] and self.gradient['b']\n",
    "        #################################################################################################\n",
    "        self.gradient['W'] = X.T @ grad\n",
    "        self.gradient['b'] = np.sum(grad, axis = 0, keepdims = True)\n",
    "        return grad@(self.params['W'].T)\n",
    "\n",
    "\n",
    "# 2. ReLU Activation\n",
    "\n",
    "\n",
    "class relu:\n",
    "\n",
    "    \"\"\"\n",
    "        The relu (rectified linear unit) module.\n",
    "\n",
    "        It is built up with NO arguments.\n",
    "        It has no parameters to learn.\n",
    "        self.mask is an attribute of relu. You can use it to store things (computed in the forward pass) for the use in the backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        \"\"\"\n",
    "            The forward pass of the relu (rectified linear unit) module.\n",
    "\n",
    "            Input:\n",
    "            - X: A numpy array of arbitrary shape.\n",
    "\n",
    "            Return:\n",
    "            - forward_output: A numpy array of the same shape of X\n",
    "        \"\"\"\n",
    "\n",
    "        ################################################################################\n",
    "        # TODO: Implement the relu forward pass. Store the result in forward_output    #\n",
    "        ################################################################################\n",
    "        forward_output = X.copy()\n",
    "        forward_output[np.where(forward_output < 0)] = 0\n",
    "        self.mask = forward_output\n",
    "        return forward_output\n",
    "\n",
    "    def backward(self, X, grad):\n",
    "\n",
    "        \"\"\"\n",
    "            The backward pass of the relu (rectified linear unit) module.\n",
    "\n",
    "            Input:\n",
    "            - X: A numpy array of arbitrary shape, the input to the forward pass.\n",
    "            - grad: A numpy array of the same shape of X, where each element is the partial derivative of the mini-batch loss\n",
    "                 w.r.t. the corresponding element in forward_output.\n",
    "\n",
    "            Return:\n",
    "            - backward_output: A numpy array of the same shape as X, where each element is the partial derivative of the mini-batch loss w.r.t. the corresponding element in  X.\n",
    "        \"\"\"\n",
    "\n",
    "        ####################################################################################################\n",
    "        # TODO: Implement the backward pass\n",
    "        # You can use the mask created in the forward step.\n",
    "        ####################################################################################################\n",
    "        self.mask[np.where( self.mask > 0 )] = 1\n",
    "        return grad * self.mask\n",
    "\n",
    "\n",
    "# 3. tanh Activation\n",
    "\n",
    "class tanh:\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        \"\"\"\n",
    "            Input:\n",
    "            - X: A numpy array of arbitrary shape.\n",
    "\n",
    "            Return:\n",
    "            - forward_output: A numpy array of the same shape of X\n",
    "        \"\"\"\n",
    "\n",
    "        ################################################################################\n",
    "        # TODO: Implement the tanh forward pass. Store the result in forward_output\n",
    "        # You can use np.tanh()\n",
    "        ################################################################################\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def backward(self, X, grad):\n",
    "\n",
    "        \"\"\"\n",
    "            Input:\n",
    "            - X: A numpy array of arbitrary shape, the input to the forward pass.\n",
    "            - grad: A numpy array of the same shape of X, where each element is the partial derivative of the mini-batch loss w.r.t. the corresponding element in forward_output.\n",
    "\n",
    "            Return:\n",
    "            - backward_output: A numpy array of the same shape as X, where each element is the partial derivative of the mini-batch loss w.r.t. the corresponding element in  X.\n",
    "        \"\"\"\n",
    "\n",
    "        ####################################################################################################\n",
    "        # TODO: Implement the backward pass\n",
    "        # Derivative of tanh is (1 - tanh^2)\n",
    "        ####################################################################################################\n",
    "        return grad*(1 - np.tanh(X)**2)\n",
    "\n",
    "\n",
    "# 4. Dropout\n",
    "\n",
    "class dropout:\n",
    "\n",
    "    \"\"\"\n",
    "        It is built up with one arguments:\n",
    "        - r: the dropout rate\n",
    "\n",
    "        It has no parameters to learn.\n",
    "        self.mask is an attribute of dropout. You can use it to store things (computed in the forward pass) for the use in the backward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, r):\n",
    "        self.r = r\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, X, is_train):\n",
    "\n",
    "        \"\"\"\n",
    "            Input:\n",
    "            - X: A numpy array of arbitrary shape.\n",
    "            - is_train: A boolean value. If False, no dropout is performed.\n",
    "\n",
    "            Operation:\n",
    "            - If p >= self.r, output that element multiplied by (1.0 / (1 - self.r)); otherwise, output 0 for that element\n",
    "\n",
    "            Return:\n",
    "            - forward_output: A numpy array of the same shape of X (the output of dropout)\n",
    "        \"\"\"\n",
    "\n",
    "        ################################################################################\n",
    "        #  TODO: We provide the forward pass to you. You only need to understand it.   #\n",
    "        ################################################################################\n",
    "\n",
    "        if is_train:\n",
    "            self.mask = (np.random.uniform(0.0, 1.0, X.shape) >= self.r).astype(float) * (1.0 / (1.0 - self.r))\n",
    "        else:\n",
    "            self.mask = np.ones(X.shape)\n",
    "        forward_output = np.multiply(X, self.mask)\n",
    "        return forward_output\n",
    "\n",
    "    def backward(self, X, grad):\n",
    "\n",
    "        \"\"\"\n",
    "            Input:\n",
    "            - X: A numpy array of arbitrary shape, the input to the forward pass.\n",
    "            - grad: A numpy array of the same shape of X, where each element is the partial derivative of the mini-batch loss w.r.t. the corresponding element in forward_output.\n",
    "\n",
    "\n",
    "            Return:\n",
    "            - backward_output: A numpy array of the same shape as X, where each element is the partial derivative of the mini-batch loss w.r.t. the corresponding element in X.\n",
    "        \"\"\"\n",
    "\n",
    "        ####################################################################################################\n",
    "        # TODO: Implement the backward pass\n",
    "        # You can use the mask created in the forward step\n",
    "        ####################################################################################################\n",
    "\n",
    "        return grad * self.mask\n",
    "\n",
    "\n",
    "\n",
    "# 5. Mini-batch Gradient Descent Optimization\n",
    "\n",
    "\n",
    "def miniBatchStochasticGradientDescent(model, momentum, _lambda, _alpha, _learning_rate):\n",
    "\n",
    "    '''\n",
    "        Input:\n",
    "            model: Dictionary containing all parameters of the model\n",
    "            momentum: Check add_momentum() function in utils.py to understand this parameter\n",
    "            _lambda: Regularization constant\n",
    "            _alpha: Momentum hyperparameter\n",
    "            _learning_rate: Learning rate for the update\n",
    "\n",
    "        Note: You can learn more about momentum here: https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/\n",
    "\n",
    "        Returns: Updated model\n",
    "    '''\n",
    "\n",
    "\n",
    "    for module_name, module in model.items():\n",
    "\n",
    "        # check if a module has learnable parameters\n",
    "        if hasattr(module, 'params'):\n",
    "            for key, _ in module.params.items():\n",
    "                g = module.gradient[key] + _lambda * module.params[key]\n",
    "\n",
    "                if _alpha > 0.0:\n",
    "\n",
    "                    #################################################################################\n",
    "                    # TODO: Update momentun using the formula:\n",
    "                    # m = alpha * m - learning_rate * g (Check add_momentum() function in utils file)\n",
    "                    # And update model parameter\n",
    "                    #################################################################################\n",
    "\n",
    "                    m = _alpha * momentum[module_name + '_' + key] - _learning_rate * g\n",
    "                    module.params[key] += m\n",
    "\n",
    "\n",
    "                else:\n",
    "\n",
    "                    #################################################################################\n",
    "                    # TODO: update model parameter without momentum\n",
    "                    #################################################################################\n",
    "\n",
    "                    module.params[key] -= module.gradient[key] * _learning_rate\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def main(main_params, optimization_type=\"minibatch_sgd\"):\n",
    "\n",
    "    # ### set the random seed ###\n",
    "    # np.random.seed(int(main_params['random_seed']))\n",
    "\n",
    "    ### data processing ###\n",
    "#     Xtrain, Ytrain, Xval, Yval , _, _ = data_loader_mnist(dataset = main_params['input_file'])\n",
    "#     N_train, d = Xtrain.shape\n",
    "#     N_val, _ = Xval.shape\n",
    "\n",
    "    index = np.arange(10)\n",
    "    unique, counts = np.unique(Ytrain, return_counts=True)\n",
    "    counts = dict(zip(unique, counts)).values()\n",
    "\n",
    "    trainSet = DataSplit(Xtrain, Ytrain)\n",
    "    valSet = DataSplit(Xval, Yval)\n",
    "\n",
    "    ### building/defining MLP ###\n",
    "    \"\"\"\n",
    "    In this script, we are going to build a MLP for a 10-class classification problem on MNIST.\n",
    "    The network structure is input --> linear --> relu --> dropout --> linear --> softmax_cross_entro\n",
    "    py loss\n",
    "    the hidden_layer size (num_L1) is 1000\n",
    "    the output_layer size (num_L2) is 10\n",
    "    \"\"\"\n",
    "    model = dict()\n",
    "    num_L1 = 1000\n",
    "    num_L2 = 10\n",
    "\n",
    "    # experimental setup\n",
    "    num_epoch = int(main_params['num_epoch'])\n",
    "    minibatch_size = int(main_params['minibatch_size'])\n",
    "    \n",
    "\n",
    "    # optimization setting: _alpha for momentum, _lambda for weight decay\n",
    "    _learning_rate = float(main_params['learning_rate'])\n",
    "    _step = 10\n",
    "    _alpha = float(main_params['alpha'])\n",
    "    _lambda = float(main_params['lambda'])\n",
    "    _dropout_rate = float(main_params['dropout_rate'])\n",
    "    _activation = main_params['activation']\n",
    "\n",
    "\n",
    "    if _activation == 'relu':\n",
    "        act = relu\n",
    "    else:\n",
    "        act = tanh\n",
    "\n",
    "    # create objects (modules) from the module classes\n",
    "    model['L1'] = linear_layer(input_D = d, output_D = num_L1)\n",
    "    model['nonlinear1'] = act()\n",
    "    model['drop1'] = dropout(r = _dropout_rate)\n",
    "    model['L2'] = linear_layer(input_D = num_L1, output_D = num_L2)\n",
    "    model['loss'] = softmax_cross_entropy()\n",
    "\n",
    "    # Momentum\n",
    "    if _alpha > 0.0:\n",
    "        momentum = add_momentum(model)\n",
    "    else:\n",
    "        momentum = None\n",
    "\n",
    "    train_acc_record = []\n",
    "    val_acc_record = []\n",
    "\n",
    "    train_loss_record = []\n",
    "    val_loss_record = []\n",
    "    \n",
    "\n",
    "    ### run training and validation ###\n",
    "    for t in range(num_epoch):\n",
    "        print('At epoch ' + str(t + 1))\n",
    "        if (t % _step == 0) and (t != 0):\n",
    "            _learning_rate = _learning_rate * 0.1\n",
    "\n",
    "        idx_order = np.random.permutation(N_train)\n",
    "\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        train_count = 0\n",
    "        \n",
    "\n",
    "        val_acc = 0.0\n",
    "        val_count = 0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        for i in range(int(np.floor(N_train / minibatch_size))):\n",
    "\n",
    "            # get a mini-batch of data\n",
    "            x, y = trainSet.get_example(idx_order[i * minibatch_size : (i + 1) * minibatch_size])\n",
    "\n",
    "            ### forward ###\n",
    "            a1 = model['L1'].forward(x)\n",
    "            h1 = model['nonlinear1'].forward(a1)\n",
    "            d1 = model['drop1'].forward(h1, is_train = True)\n",
    "            a2 = model['L2'].forward(d1)\n",
    "            loss = model['loss'].forward(a2, y)\n",
    "\n",
    "\n",
    "            ### backward ###\n",
    "            ######################################################################################\n",
    "            # TODO: Call the backward methods of every layer in the model in reverse order\n",
    "            # We have given the first and last backward calls\n",
    "            # Do not modify them.\n",
    "            ######################################################################################\n",
    "            grad_a2 = model['loss'].backward(a2, y)\n",
    "            grad_d1 = model['L2'].backward(d1, grad_a2)\n",
    "            grad_h1 = model['drop1'].backward(h1 ,grad_d1)\n",
    "            grad_a1 = model['nonlinear1'].backward(a1,grad_h1)\n",
    "            ######################################################################################\n",
    "            # NOTE: DO NOT MODIFY CODE BELOW THIS, until next TODO\n",
    "            ######################################################################################\n",
    "            grad_x = model['L1'].backward(x, grad_a1)\n",
    "\n",
    "            ### gradient_update ###\n",
    "            model = miniBatchStochasticGradientDescent(model, momentum, _lambda, _alpha, _learning_rate)\n",
    "\n",
    "        ### Computing training accuracy and obj ###\n",
    "        for i in range(int(np.floor(N_train / minibatch_size))):\n",
    "\n",
    "            x, y = trainSet.get_example(np.arange(i * minibatch_size, (i + 1) * minibatch_size))\n",
    "\n",
    "            ### forward ###\n",
    "            ######################################################################################\n",
    "            # TODO: Call the forward methods of every layer in the model in order\n",
    "            # Check above forward code\n",
    "            # Make sure to keep train as False\n",
    "            ######################################################################################\n",
    "\n",
    "            a1 = model['L1'].forward(x)\n",
    "            h1 = model['nonlinear1'].forward(a1)\n",
    "            d1 = model['drop1'].forward(h1, is_train = False)\n",
    "            a2 = model['L2'].forward(d1)\n",
    "\n",
    "            ######################################################################################\n",
    "            # NOTE: DO NOT MODIFY CODE BELOW THIS, until next TODO\n",
    "            ######################################################################################\n",
    "\n",
    "            loss = model['loss'].forward(a2, y)\n",
    "            train_loss += loss\n",
    "            train_acc += np.sum(predict_label(a2) == y)\n",
    "            train_count += len(y)\n",
    "\n",
    "        train_loss = train_loss\n",
    "        train_acc = train_acc / train_count\n",
    "        train_acc_record.append(train_acc)\n",
    "        train_loss_record.append(train_loss)\n",
    "\n",
    "        print('Training loss at epoch ' + str(t + 1) + ' is ' + str(train_loss))\n",
    "        print('Training accuracy at epoch ' + str(t + 1) + ' is ' + str(train_acc))\n",
    "\n",
    "        ### Computing validation accuracy ###\n",
    "        for i in range(int(np.floor(N_val / minibatch_size))):\n",
    "\n",
    "            x, y = valSet.get_example(np.arange(i * minibatch_size, (i + 1) * minibatch_size))\n",
    "\n",
    "            ### forward ###\n",
    "            ######################################################################################\n",
    "            # TODO: Call the forward methods of every layer in the model in order\n",
    "            # Check above forward code\n",
    "            # Make sure to keep train as False\n",
    "            ######################################################################################\n",
    "\n",
    "            a1 = model['L1'].forward(x)\n",
    "            h1 = model['nonlinear1'].forward(a1)\n",
    "            d1 = model['drop1'].forward(h1, is_train = False)\n",
    "            a2 = model['L2'].forward(d1)\n",
    "\n",
    "\n",
    "            ######################################################################################\n",
    "            # NOTE: DO NOT MODIFY CODE BELOW THIS, until next TODO\n",
    "            ######################################################################################\n",
    "\n",
    "            loss = model['loss'].forward(a2, y)\n",
    "            val_loss += loss\n",
    "            val_acc += np.sum(predict_label(a2) == y)\n",
    "            val_count += len(y)\n",
    "\n",
    "        val_loss_record.append(val_loss)\n",
    "        val_acc = val_acc / val_count\n",
    "        val_acc_record.append(val_acc)\n",
    "\n",
    "        print('Validation accuracy at epoch ' + str(t + 1) + ' is ' + str(val_acc))\n",
    "\n",
    "    # save file\n",
    "    json.dump({'train': train_acc_record, 'val': val_acc_record},\n",
    "              open('MLP_lr' + str(main_params['learning_rate']) +\n",
    "                   '_m' + str(main_params['alpha']) +\n",
    "                   '_w' + str(main_params['lambda']) +\n",
    "                   '_d' + str(main_params['dropout_rate']) +\n",
    "                   '_a' + str(main_params['activation']) +\n",
    "                   '.json', 'w'))\n",
    "\n",
    "    print('Finish running!')\n",
    "    return train_loss_record, val_loss_record\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    ######################################################################################\n",
    "    # Do not change this part of the code.\n",
    "    # These are the default arguments used to run your code.\n",
    "    # These parameters will be changed while grading.\n",
    "    ######################################################################################\n",
    "\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--random_seed', default=42)\n",
    "#     parser.add_argument('--learning_rate', default=0.01)\n",
    "#     parser.add_argument('--alpha', default=0.0)\n",
    "#     parser.add_argument('--lambda', default=0.0)\n",
    "#     parser.add_argument('--dropout_rate', default=0.0)\n",
    "#     parser.add_argument('--num_epoch', default=10)\n",
    "#     parser.add_argument('--minibatch_size', default=5)\n",
    "#     parser.add_argument('--activation', default='relu')\n",
    "# #     parser.add_argument('--input_file', default='mnist_subset.json')\n",
    "#     args = parser.parse_args()\n",
    "#     main_params = vars(args)\n",
    "#     main(main_params)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 1 3 2 4 7 5 0 9 6]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[8 1]\n",
      "[3 2 4 7 5 0 9 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.permutation(10)\n",
    "b = np.arange(10)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "b = b[a]\n",
    "print(b[:2])\n",
    "print(b[2:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'relu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-95c83e4d1330>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'relu' is not defined"
     ]
    }
   ],
   "source": [
    "act = relu\n",
    "act()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
